{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from helper import get_news_dataset,get_data, create_classes\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE THE DATASETS ARE CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![caption](files/requirements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AABABUTdx7YqCeBquA1Ky7z8a/The%20SUM%20dataset?dl=1#\"\n",
    "news_url  = \"https://www.dropbox.com/sh/euppz607r6gsen2/AACq4aMWDOIw2I_SSGqJ-r2Oa/Online%20News%20Popularity%20(Mashable%20News)?dl=1\"\n",
    "housing_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AAD6JGlvG5XADIjg9SCojvpya/House%20Sales%20in%20King%20County%2C%20USA?dl=1&preview=kc_house_data.csv\"\n",
    "all_urls = [sumdata_url,news_url,housing_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(all_urls) # retrieves the data if there is NO data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_noise_path = \"data/with noise/The SUM dataset, with noise.csv\"\n",
    "sumdata_path = \"data/without noise/The SUM dataset, without noise.csv\" \n",
    "housing_price_path =\"data/kc_house_data.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sum_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Noisy Target</th>\n",
       "      <th>Noisy Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>66957</td>\n",
       "      <td>74432</td>\n",
       "      <td>96087</td>\n",
       "      <td>103120</td>\n",
       "      <td>64272</td>\n",
       "      <td>150633</td>\n",
       "      <td>181787</td>\n",
       "      <td>180349</td>\n",
       "      <td>216912</td>\n",
       "      <td>304071</td>\n",
       "      <td>1434819</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>96030</td>\n",
       "      <td>86875</td>\n",
       "      <td>108299</td>\n",
       "      <td>148025</td>\n",
       "      <td>16965</td>\n",
       "      <td>253819</td>\n",
       "      <td>258672</td>\n",
       "      <td>268851</td>\n",
       "      <td>404599</td>\n",
       "      <td>543092</td>\n",
       "      <td>2148748</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0         1      66957      74432      96087     103120   \n",
       "1         2      96030      86875     108299     148025   \n",
       "\n",
       "   Feature 5 (meaningless)  Feature 6  Feature 7  Feature 8  Feature 9  \\\n",
       "0                    64272     150633     181787     180349     216912   \n",
       "1                    16965     253819     258672     268851     404599   \n",
       "\n",
       "   Feature 10  Noisy Target Noisy Target Class  \n",
       "0      304071       1434819  Very Large Number  \n",
       "1      543092       2148748  Very Large Number  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata_noise = pd.read_csv(sumdata_noise_path, delimiter=\";\") \n",
    "sumdata_noise.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sum_noise dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_noise_reg_Y = sumdata_noise['Noisy Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as regression target \n",
    "le = LabelEncoder() \n",
    "sumdata_noise_classif_Y = le.fit_transform(sumdata_noise['Noisy Target Class'])\n",
    "\n",
    "# Use rest columns as explananatory variables \n",
    "sumdata_noise_X = sumdata_noise.iloc[:, 1:-2].values\n",
    "# first column is instance (just a row number), and the last two features are target class and target\n",
    " \n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "sumdata_noise_X = scX.fit_transform(sumdata_noise_X)\n",
    "sumdata_noise_reg_Y = scY.fit_transform(sumdata_noise_reg_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sumdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>57326</td>\n",
       "      <td>68791</td>\n",
       "      <td>82549</td>\n",
       "      <td>99059</td>\n",
       "      <td>72624</td>\n",
       "      <td>142645</td>\n",
       "      <td>171174</td>\n",
       "      <td>205409</td>\n",
       "      <td>246491</td>\n",
       "      <td>295789</td>\n",
       "      <td>1073444</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>87859</td>\n",
       "      <td>105431</td>\n",
       "      <td>126517</td>\n",
       "      <td>151820</td>\n",
       "      <td>19982</td>\n",
       "      <td>218621</td>\n",
       "      <td>262345</td>\n",
       "      <td>314814</td>\n",
       "      <td>377777</td>\n",
       "      <td>453332</td>\n",
       "      <td>1645184</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0         1      57326      68791      82549      99059   \n",
       "1         2      87859     105431     126517     151820   \n",
       "\n",
       "   Feature 5 (meaningless)  Feature 6  Feature 7  Feature 8  Feature 9  \\\n",
       "0                    72624     142645     171174     205409     246491   \n",
       "1                    19982     218621     262345     314814     377777   \n",
       "\n",
       "   Feature 10   Target       Target Class  \n",
       "0      295789  1073444  Very Large Number  \n",
       "1      453332  1645184  Very Large Number  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata = pd.read_csv(sumdata_path, delimiter=\";\")  \n",
    "sumdata.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sumdata dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Target' as regression target\n",
    "- Extract 'Target Class' as classification target\n",
    "- Extract rest of the columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_reg_Y = sumdata['Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as classification target\n",
    "le = LabelEncoder() \n",
    "sumdata_classif_Y = le.fit_transform(sumdata['Target Class'])\n",
    "\n",
    "# Use rest of columns as explananatory variables \n",
    "sumdata_X = sumdata.iloc[:, 1:-2].values\n",
    "\n",
    " \n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "sumdata_X      = scX.fit_transform(sumdata_X)\n",
    "sumdata_reg_Y  = scX.fit_transform(sumdata_reg_Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load House Price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>7237550310</td>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1321400060</td>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2008000270</td>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2414600126</td>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3793500160</td>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id             date      price  bedrooms  bathrooms  sqft_living  \\\n",
       "0  7129300520  20141013T000000   221900.0         3       1.00         1180   \n",
       "1  6414100192  20141209T000000   538000.0         3       2.25         2570   \n",
       "2  5631500400  20150225T000000   180000.0         2       1.00          770   \n",
       "3  2487200875  20141209T000000   604000.0         4       3.00         1960   \n",
       "4  1954400510  20150218T000000   510000.0         3       2.00         1680   \n",
       "5  7237550310  20140512T000000  1225000.0         4       4.50         5420   \n",
       "6  1321400060  20140627T000000   257500.0         3       2.25         1715   \n",
       "7  2008000270  20150115T000000   291850.0         3       1.50         1060   \n",
       "8  2414600126  20150415T000000   229500.0         3       1.00         1780   \n",
       "9  3793500160  20150312T000000   323000.0         3       2.50         1890   \n",
       "\n",
       "   sqft_lot  floors  waterfront  view  condition  grade  sqft_above  \\\n",
       "0      5650     1.0           0     0          3      7        1180   \n",
       "1      7242     2.0           0     0          3      7        2170   \n",
       "2     10000     1.0           0     0          3      6         770   \n",
       "3      5000     1.0           0     0          5      7        1050   \n",
       "4      8080     1.0           0     0          3      8        1680   \n",
       "5    101930     1.0           0     0          3     11        3890   \n",
       "6      6819     2.0           0     0          3      7        1715   \n",
       "7      9711     1.0           0     0          3      7        1060   \n",
       "8      7470     1.0           0     0          3      7        1050   \n",
       "9      6560     2.0           0     0          3      7        1890   \n",
       "\n",
       "   sqft_basement  yr_built  yr_renovated  zipcode      lat     long  \\\n",
       "0              0      1955             0    98178  47.5112 -122.257   \n",
       "1            400      1951          1991    98125  47.7210 -122.319   \n",
       "2              0      1933             0    98028  47.7379 -122.233   \n",
       "3            910      1965             0    98136  47.5208 -122.393   \n",
       "4              0      1987             0    98074  47.6168 -122.045   \n",
       "5           1530      2001             0    98053  47.6561 -122.005   \n",
       "6              0      1995             0    98003  47.3097 -122.327   \n",
       "7              0      1963             0    98198  47.4095 -122.315   \n",
       "8            730      1960             0    98146  47.5123 -122.337   \n",
       "9              0      2003             0    98038  47.3684 -122.031   \n",
       "\n",
       "   sqft_living15  sqft_lot15  \n",
       "0           1340        5650  \n",
       "1           1690        7639  \n",
       "2           2720        8062  \n",
       "3           1360        5000  \n",
       "4           1800        7503  \n",
       "5           4760      101930  \n",
       "6           2238        6819  \n",
       "7           1650        9711  \n",
       "8           1780        8113  \n",
       "9           2390        7570  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_price = pd.read_csv(housing_price_path, delimiter=\",\")\n",
    "housing_price.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess housing price dataset\n",
    "\n",
    "- Remove 'Id' as it simply represents the row number\n",
    "- Use 'price' as regression target\n",
    "- Use 'SaleCondition' as classification target\n",
    "- For explananatory variables, use the following numerical variable and categorical variables\n",
    "    - Numerical\n",
    "        - bedrooms\n",
    "        - bathrooms\n",
    "        - sqft_living\n",
    "        - sqft_lot\n",
    "        - condition\n",
    "        - sqft_above\n",
    "        - yr_built\n",
    "        - sqft_living15\n",
    "        - sqft_lot15\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.39873715, -1.44746357, -0.97983502, ..., -0.54489777,\n",
       "        -0.9433552 , -0.26071541],\n",
       "       [-0.39873715,  0.1756067 ,  0.53363434, ..., -0.6810785 ,\n",
       "        -0.43268619, -0.18786773],\n",
       "       [-1.47395936, -1.44746357, -1.42625404, ..., -1.29389179,\n",
       "         1.07013975, -0.17237524],\n",
       "       ..., \n",
       "       [-1.47395936, -1.77207762, -1.15404732, ...,  1.29354209,\n",
       "        -1.41025258, -0.39414129],\n",
       "       [-0.39873715,  0.50022075, -0.52252773, ...,  1.12331618,\n",
       "        -0.8412214 , -0.42051149],\n",
       "       [-1.47395936, -1.77207762, -1.15404732, ...,  1.25949691,\n",
       "        -1.41025258, -0.41794772]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use 'price' as regression target\n",
    "housing_price_reg_Y = housing_price.loc[:, ['price']]\n",
    "\n",
    "# Use following features to predict price\n",
    "features = ['bedrooms', 'bathrooms', 'sqft_living', 'sqft_lot', 'condition', 'sqft_above', 'yr_built',\n",
    "                      'sqft_living15', 'sqft_lot15']\n",
    "\n",
    "housing_price[features[0]]\n",
    "filtered_table = housing_price[features[0]]\n",
    "filtered_table\n",
    "\n",
    "for c in features[1:]:\n",
    "    filtered_table = pd.concat([filtered_table, housing_price[c]], axis=1)\n",
    "filtered_table\n",
    "\n",
    "# get explanatory variable\n",
    "housing_price_reg_X = filtered_table[:].values\n",
    "# housing_price_classif_X = filtered_table[:].values\n",
    "\n",
    "## get the regression target and classification target\n",
    "\n",
    "# I am encoding price as categorical variable as the classification target, e.g try to determine if the house condition\n",
    "housing_price_classif_Y = np.zeros(housing_price.shape[0])\n",
    "house_price_mean = housing_price['price'].values.mean()\n",
    "for i, r in enumerate(housing_price['price']):\n",
    "    if r > house_price_mean:\n",
    "        housing_price_classif_Y[i] = 1\n",
    "    else:\n",
    "        housing_price_classif_Y[i] = 0\n",
    "        \n",
    "# I am using price as the regression target\n",
    "housing_price_reg_Y = housing_price['price'].values.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "# Apply Feature Scaling to the classification variable\n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "housing_price_classif_X = scX.fit_transform(housing_price_reg_X[:])\n",
    "housing_price_reg_X = scX.fit_transform(housing_price_reg_X)\n",
    "housing_price_reg_Y = scY.fit_transform(housing_price_reg_Y)\n",
    "housing_price_reg_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  \\\n",
       "0                   0.815385         4.0              2.0        1.0   \n",
       "\n",
       "    num_videos   average_token_length   num_keywords  \\\n",
       "0          0.0               4.680365            5.0   \n",
       "\n",
       "    data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                         0.0                             1.0   \n",
       "\n",
       "    data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                   0.0                      0.0                    0.0   \n",
       "\n",
       "    data_channel_is_world   kw_min_min   kw_max_min   kw_avg_min   kw_min_max  \\\n",
       "0                     0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    kw_max_max   kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    self_reference_min_shares   self_reference_max_shares  \\\n",
       "0                       496.0                       496.0   \n",
       "\n",
       "    self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
       "0                        496.0                 1.0                  0.0   \n",
       "\n",
       "    weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
       "0                    0.0                   0.0                 0.0   \n",
       "\n",
       "    weekday_is_saturday   weekday_is_sunday   is_weekend    LDA_00    LDA_01  \\\n",
       "0                   0.0                 0.0          0.0  0.500331  0.378279   \n",
       "\n",
       "     LDA_02    LDA_03    LDA_04   global_subjectivity  \\\n",
       "0  0.040005  0.041263  0.040123              0.521617   \n",
       "\n",
       "    global_sentiment_polarity   global_rate_positive_words  \\\n",
       "0                    0.092562                     0.045662   \n",
       "\n",
       "    global_rate_negative_words   rate_positive_words   rate_negative_words  \\\n",
       "0                     0.013699              0.769231              0.230769   \n",
       "\n",
       "    avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
       "0                0.378636                     0.1                     0.7   \n",
       "\n",
       "    avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
       "0                   -0.35                    -0.6                    -0.2   \n",
       "\n",
       "    title_subjectivity   title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  0.5                    -0.1875                      0.0   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                         0.1875      593  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = get_news_dataset() \n",
    "news.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess news dataset\n",
    "- Below are the features  that will be selected \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns= [ ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
    "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
    "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
    "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
    "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
    "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
    "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
    "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
    "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
    "       ' self_reference_max_shares', ' self_reference_avg_sharess', ' global_subjectivity', ' global_rate_positive_words',\n",
    "       ' global_rate_negative_words', ' rate_positive_words',\n",
    "       ' rate_negative_words', ' avg_positive_polarity',\n",
    "       ' min_positive_polarity', ' max_positive_polarity',' title_subjectivity', ' abs_title_subjectivity',\n",
    "       ' abs_title_sentiment_polarity', ' shares']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "- 0 - 10            class 1\n",
    "- 10 - 100          class 2 \n",
    "- 101 - 1000        class 3\n",
    "- 1001 - 10,000     class 4\n",
    "- 10,001 - 100,000  class 5\n",
    "- 100,001 -         class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = news[columns] \n",
    "# Classification the target values \n",
    "news['label'] = news.apply(create_classes, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "news_reg_Y = news[' shares'].values.reshape(-1, 1)\n",
    "\n",
    "news_reg_X = news.iloc[:, 0:-2].values # the last 2 columns are the target values \n",
    "# no need to label encode as they already encoded\n",
    "news_classif_Y = news['label'] \n",
    "\n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "news_reg_X  = scX.fit_transform(news_reg_X)\n",
    "news_reg_Y  = scX.fit_transform(news_reg_Y)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits Algorithms to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000,\n",
    "1000000, 5000000, 10000000, 50000000, 100000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def root_mean_square_error(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model( X, y, dataset_name, algorithm, isReg, algo_params={}): \n",
    "    \n",
    "    print (\"Algorithm: {}\\nDataset: {}\\n\".format( algorithm.__name__, dataset_name))\n",
    "    for chunk in data_chunks:\n",
    "        \n",
    "        # if chunk is greater than the no. of examples quite from the chunking\n",
    "        if chunk > X.shape[0]: \n",
    "            break\n",
    "        \n",
    "        print (\"Chunk Size: {}\\n\".format(chunk))\n",
    "        \n",
    "        # generate the chunk file\n",
    "        current_X = X[0:chunk]\n",
    "        current_y = y[0:chunk]\n",
    "        \n",
    "        kFoldModelling(current_X, current_y, 10, algorithm, isReg, algo_params)\n",
    "        \n",
    "         \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFoldModelling (X, y, kfolds, algorithm, isReg, algo_params):\n",
    "    \n",
    "    kf = KFold(n_splits=kfolds, shuffle=True)\n",
    "    rmse = np.zeros((10,1))\n",
    "    mae = np.zeros((10,1))\n",
    "    accuracy = np.zeros((10,1))\n",
    "    precision = np.zeros((10,1))\n",
    "    \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "      \n",
    "        # fit the model to the datset\n",
    "        lm = algorithm(**algo_params)\n",
    "        lm.fit(X_train, y_train)\n",
    "        \n",
    "        if isReg:     \n",
    "            rmse[i] = root_mean_square_error(y_test, lm.predict(X_test))  # RMSE https://www.kaggle.com/wiki/RootMeanSquaredError\n",
    "            mae[i] = mean_absolute_error(y_test, lm.predict(X_test))\n",
    "        else:\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy[i] = accuracy_score(y_test, lm.predict(X_test))\n",
    "            \n",
    "            precision[i] = precision_score(y_test, lm.predict(X_test),average=\"weighted\")\n",
    "        # print the result, we will need to have method that genereates the result csv file required.\n",
    "    \n",
    "    if isReg:     \n",
    "        print (\"Iteration: {}\\nRMSE: {}\\nMAE: {}\\n\".format( i, rmse.mean(), mae.mean()))\n",
    "    else:\n",
    "        print (\"Iteration: {}\\Accuracy: {}\\nPrecision: {}\\n\".format( i, accuracy.mean(), precision.mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fits Regression Algorithms to datasets\n",
    "\n",
    "    - Linear Regression\n",
    "    - Random Forest Regression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: LinearRegression\n",
      "Dataset: House dataset\n",
      "\n",
      "Chunk Size: 100\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.6647217581335125\n",
      "MAE: 0.4907503364653518\n",
      "\n",
      "Chunk Size: 500\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.633835735238472\n",
      "MAE: 0.4250613211814353\n",
      "\n",
      "Chunk Size: 1000\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.5966280460993664\n",
      "MAE: 0.4068109395465417\n",
      "\n",
      "Chunk Size: 5000\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.6894764906664024\n",
      "MAE: 0.4402858540189084\n",
      "\n",
      "Chunk Size: 10000\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.6680853786524494\n",
      "MAE: 0.42950714804040435\n",
      "\n",
      "Chunk Size: 21613\n",
      "\n",
      "Iteration: 9\n",
      "RMSE: 0.6588125051124283\n",
      "MAE: 0.433497291354014\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model(sumdata_noise_X, sumdata_noise_reg_Y, \"The Sum Dataset(with noise)\", LinearRegression, True)\n",
    "# model(sumdata_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\",  LinearRegression, True) \n",
    "# model(news_reg_X, news_reg_Y, \"News dataset\", LinearRegression, True)\n",
    "model(housing_price_reg_X, housing_price_reg_Y, \"House dataset\", LinearRegression, True)\n",
    "\n",
    "\n",
    "# model(sumdata_noise_X, sumdata_noise_reg_Y,\"The Sum Dataset(with noise)\",  RandomForestRegressor, True)\n",
    "# model(sumdata_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\", RandomForestRegressor, True) \n",
    "# model(news_reg_X, news_reg_Y, \"News dataset\", RandomForestRegressor, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: LogisticRegression\n",
      "Dataset: House dataset\n",
      "\n",
      "Chunk Size: 100\n",
      "\n",
      "Iteration: 9\\Accuracy: 0.8\n",
      "Precision: 0.8118333333333332\n",
      "\n",
      "Chunk Size: 500\n",
      "\n",
      "Iteration: 9\\Accuracy: 0.808\n",
      "Precision: 0.8082546136760685\n",
      "\n",
      "Chunk Size: 1000\n",
      "\n",
      "Iteration: 9\\Accuracy: 0.8229999999999998\n",
      "Precision: 0.8236591843018337\n",
      "\n",
      "Chunk Size: 5000\n",
      "\n",
      "Iteration: 9\\Accuracy: 0.8089999999999999\n",
      "Precision: 0.8074454614646053\n",
      "\n",
      "Chunk Size: 10000\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\\Accuracy: 0.8091000000000002\n",
      "Precision: 0.8068040859293317\n",
      "\n",
      "Chunk Size: 21613\n",
      "\n",
      "Iteration: 9\\Accuracy: 0.8047017154236592\n",
      "Precision: 0.8025499633711558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model(sumdata_noise_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", LogisticRegression, False)\n",
    "# model(sumdata_noise_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", LogisticRegression, False)\n",
    "#model(news_reg_X, news_classif_Y, \"News dataset\", LogisticRegression, False)\n",
    "model(housing_price_classif_X, housing_price_classif_Y, \"House dataset\", LogisticRegression, False)\n",
    " \n",
    "# from sklearn.svm import SVC\n",
    "# model(sumdata_noise_classi_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", SVC, False)\n",
    "# model(sumdata_classi_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", SVC, False)\n",
    "# model(housing_price_classi_X, housing_price_classif_Y, \"Housing Dataset\", SVC, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
