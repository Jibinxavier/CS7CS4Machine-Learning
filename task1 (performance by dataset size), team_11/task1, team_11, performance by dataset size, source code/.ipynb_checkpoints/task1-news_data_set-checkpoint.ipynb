{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from helper import get_news_dataset,get_data, create_classes\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Preprocessing libraries\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from math import sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE THE DATASETS ARE CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![caption](files/requirements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AABABUTdx7YqCeBquA1Ky7z8a/The%20SUM%20dataset?dl=1#\"\n",
    "news_url  = \"https://www.dropbox.com/sh/euppz607r6gsen2/AACq4aMWDOIw2I_SSGqJ-r2Oa/Online%20News%20Popularity%20(Mashable%20News)?dl=1\"\n",
    "housing_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AAD6JGlvG5XADIjg9SCojvpya/House%20Sales%20in%20King%20County%2C%20USA?dl=1&preview=kc_house_data.csv\"\n",
    "all_urls = [sumdata_url,news_url,housing_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(all_urls) # retrieves the data if there is NO data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_noise_path = \"data/with noise/The SUM dataset, with noise.csv\"\n",
    "sumdata_path = \"data/without noise/The SUM dataset, without noise.csv\" \n",
    "housing_price_path =\"data/kc_house_data.csv\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sum_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless but please still use it)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Noisy Target</th>\n",
       "      <th>Noisy Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>62485</td>\n",
       "      <td>58472</td>\n",
       "      <td>84200</td>\n",
       "      <td>86181</td>\n",
       "      <td>75529</td>\n",
       "      <td>136939</td>\n",
       "      <td>150633</td>\n",
       "      <td>230058</td>\n",
       "      <td>246491</td>\n",
       "      <td>257336</td>\n",
       "      <td>1352179</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>75559</td>\n",
       "      <td>119137</td>\n",
       "      <td>146760</td>\n",
       "      <td>139674</td>\n",
       "      <td>19582</td>\n",
       "      <td>177083</td>\n",
       "      <td>217746</td>\n",
       "      <td>321110</td>\n",
       "      <td>434444</td>\n",
       "      <td>516798</td>\n",
       "      <td>1976446</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0         1      62485      58472      84200      86181   \n",
       "1         2      75559     119137     146760     139674   \n",
       "\n",
       "   Feature 5 (meaningless but please still use it)  Feature 6  Feature 7  \\\n",
       "0                                            75529     136939     150633   \n",
       "1                                            19582     177083     217746   \n",
       "\n",
       "   Feature 8  Feature 9  Feature 10  Noisy Target Noisy Target Class  \n",
       "0     230058     246491      257336       1352179  Very Large Number  \n",
       "1     321110     434444      516798       1976446  Very Large Number  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata_noise = pd.read_csv(sumdata_noise_path, delimiter=\";\") \n",
    "sumdata_noise.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sum_noise dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_noise_reg_Y = sumdata_noise['Noisy Target'].values.reshape(-1, 1)\n",
    "le = LabelEncoder() \n",
    "# Use 'Nosiy Target Class' Large Number as regression target \n",
    "sumdata_noise_classif_Y = le.fit_transform(sumdata_noise['Noisy Target Class'])\n",
    "\n",
    "# Use rest columns as explananatory variables \n",
    "sumdata_noise_X = sumdata_noise.iloc[:, 1:-2].values\n",
    "# first column is instance (just a row number), and the last two features are target class and target\n",
    " \n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "sumdata_noise_X = scX.fit_transform(sumdata_noise_X)\n",
    "sumdata_noise_reg_Y = scY.fit_transform(sumdata_noise_reg_Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sumdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless but please still use it)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>57326</td>\n",
       "      <td>68791</td>\n",
       "      <td>82549</td>\n",
       "      <td>99059</td>\n",
       "      <td>72624</td>\n",
       "      <td>142645</td>\n",
       "      <td>171174</td>\n",
       "      <td>205409</td>\n",
       "      <td>246491</td>\n",
       "      <td>295789</td>\n",
       "      <td>1369233</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>87859</td>\n",
       "      <td>105431</td>\n",
       "      <td>126517</td>\n",
       "      <td>151820</td>\n",
       "      <td>19982</td>\n",
       "      <td>218621</td>\n",
       "      <td>262345</td>\n",
       "      <td>314814</td>\n",
       "      <td>377777</td>\n",
       "      <td>453332</td>\n",
       "      <td>2098516</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0         1      57326      68791      82549      99059   \n",
       "1         2      87859     105431     126517     151820   \n",
       "\n",
       "   Feature 5 (meaningless but please still use it)  Feature 6  Feature 7  \\\n",
       "0                                            72624     142645     171174   \n",
       "1                                            19982     218621     262345   \n",
       "\n",
       "   Feature 8  Feature 9  Feature 10   Target       Target Class  \n",
       "0     205409     246491      295789  1369233  Very Large Number  \n",
       "1     314814     377777      453332  2098516  Very Large Number  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata = pd.read_csv(sumdata_path, delimiter=\";\")  \n",
    "sumdata.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sumdata dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Target' as regression target\n",
    "- Extract 'Target Class' as classification target\n",
    "- Extract rest of the columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_reg_Y = sumdata['Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as classification target\n",
    "le = LabelEncoder() \n",
    "sumdata_classif_Y = le.fit_transform(sumdata['Target Class'])\n",
    "\n",
    "# Use rest columns as explananatory variables \n",
    "sumdata_X = sumdata.iloc[:, 1:-2].values\n",
    "\n",
    " \n",
    "\n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "sumdata_X      = scX.fit_transform(sumdata_X)\n",
    "sumdata_reg_Y  = scX.fit_transform(sumdata_reg_Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  \\\n",
       "0                   0.815385         4.0              2.0        1.0   \n",
       "\n",
       "    num_videos   average_token_length   num_keywords  \\\n",
       "0          0.0               4.680365            5.0   \n",
       "\n",
       "    data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                         0.0                             1.0   \n",
       "\n",
       "    data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                   0.0                      0.0                    0.0   \n",
       "\n",
       "    data_channel_is_world   kw_min_min   kw_max_min   kw_avg_min   kw_min_max  \\\n",
       "0                     0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    kw_max_max   kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    self_reference_min_shares   self_reference_max_shares  \\\n",
       "0                       496.0                       496.0   \n",
       "\n",
       "    self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
       "0                        496.0                 1.0                  0.0   \n",
       "\n",
       "    weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
       "0                    0.0                   0.0                 0.0   \n",
       "\n",
       "    weekday_is_saturday   weekday_is_sunday   is_weekend    LDA_00    LDA_01  \\\n",
       "0                   0.0                 0.0          0.0  0.500331  0.378279   \n",
       "\n",
       "     LDA_02    LDA_03    LDA_04   global_subjectivity  \\\n",
       "0  0.040005  0.041263  0.040123              0.521617   \n",
       "\n",
       "    global_sentiment_polarity   global_rate_positive_words  \\\n",
       "0                    0.092562                     0.045662   \n",
       "\n",
       "    global_rate_negative_words   rate_positive_words   rate_negative_words  \\\n",
       "0                     0.013699              0.769231              0.230769   \n",
       "\n",
       "    avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
       "0                0.378636                     0.1                     0.7   \n",
       "\n",
       "    avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
       "0                   -0.35                    -0.6                    -0.2   \n",
       "\n",
       "    title_subjectivity   title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  0.5                    -0.1875                      0.0   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                         0.1875      593  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = get_news_dataset() \n",
    "news.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess news dataset\n",
    "- Below are the features  that will be selected \n",
    "-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns= [ ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
    "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
    "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
    "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
    "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
    "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
    "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
    "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
    "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
    "       ' self_reference_max_shares', ' self_reference_avg_sharess', ' global_subjectivity', ' global_rate_positive_words',\n",
    "       ' global_rate_negative_words', ' rate_positive_words',\n",
    "       ' rate_negative_words', ' avg_positive_polarity',\n",
    "       ' min_positive_polarity', ' max_positive_polarity',' title_subjectivity', ' abs_title_subjectivity',\n",
    "       ' abs_title_sentiment_polarity', ' shares']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "- 0 - 10            class 1\n",
    "- 10 - 100          class 2 \n",
    "- 101 - 1000        class 3\n",
    "- 1001 - 10,000     class 4\n",
    "- 10,001 - 100,000  class 5\n",
    "- 100,001 -         class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "news = news[columns] \n",
    "# Classification the target values \n",
    "news['label'] = news.apply(create_classes, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "news_reg_Y = news[' shares'].values.reshape(-1, 1)\n",
    "\n",
    "news_reg_X = news.iloc[:, 0:-2].values # the last 2 columns are the target values \n",
    "# no need to label encode as they already encoded\n",
    "news_classif_Y = news['label'] \n",
    "\n",
    "scX = StandardScaler() #standardising features\n",
    "scY = StandardScaler() # standardising y \n",
    "news_reg_X  = scX.fit_transform(news_reg_X)\n",
    "news_reg_Y  = scX.fit_transform(news_reg_Y)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits Algorithms to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000,\n",
    "1000000, 5000000, 10000000, 50000000, 100000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def root_mean_square_error(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model( X, y, dataset_name, algorithm, isReg): \n",
    "    \n",
    "    print (\"Algorithm: {}\\nDataset: {}\\n\".format( algorithm.__name__, dataset_name))\n",
    "    for chunk in data_chunks:\n",
    "        \n",
    "        # if chunk is greater than the no. of examples quite from the chunking\n",
    "        if chunk > X.shape[0]: \n",
    "            chunk = X.shape[0]\n",
    "        \n",
    "        print (\"Chunk Size: {}\\n\".format(chunk))\n",
    "        \n",
    "        # generate the chunk file\n",
    "        current_X = X[0:chunk]\n",
    "        current_y = y[0:chunk]\n",
    "        \n",
    "        kFoldModelling(current_X, current_y, 10, algorithm, isReg)\n",
    "        \n",
    "        if chunk == X.shape[0]:\n",
    "            break\n",
    "              \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFoldModelling (X, y, kfolds, algorithm, isReg):\n",
    "    \n",
    "    kf = KFold(n_splits=kfolds, shuffle=True)\n",
    "    rmse = np.zeros((10,1))\n",
    "    mae = np.zeros((10,1))\n",
    "    accuracy = np.zeros((10,1))\n",
    "    precision = np.zeros((10,1))\n",
    "    \n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "      \n",
    "        # fit the model to the datset\n",
    "        lm = algorithm(multi_class=\"multinomial\",solver='lbfgs' )\n",
    "        lm.fit(X_train, y_train)\n",
    "        \n",
    "        if isReg:     \n",
    "            rmse[i] = root_mean_square_error(y_test, lm.predict(X_test))  # RMSE https://www.kaggle.com/wiki/RootMeanSquaredError\n",
    "            mae[i] = mean_absolute_error(y_test, lm.predict(X_test))\n",
    "        else:\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy[i] = accuracy_score(y_test, lm.predict(X_test))\n",
    "            \n",
    "            precision[i] = precision_score(y_test, lm.predict(X_test),average=\"weighted\")\n",
    "        # print the result, we will need to have method that genereates the result csv file required.\n",
    "    \n",
    "    if isReg:     \n",
    "        print (\"Iteration: {}\\nRMSE: {}\\nMAE: {}\\n\".format( i, rmse.mean(), mae.mean()))\n",
    "    else:\n",
    "        print (\"Iteration: {}\\Accuracy: {}\\nPrecision: {}\\n\".format( i, accuracy.mean(), precision.mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fits Regression Algorithms to datasets\n",
    "\n",
    "    - Linear Regression\n",
    "    - Random Forest Regression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algorithm: LogisticRegression\n",
      "Dataset: News dataset\n",
      "\n",
      "Chunk Size: 100\n",
      "\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 9\\Accuracy: 0.48999999999999994\n",
      "Precision: 0.5284880952380953\n",
      "\n",
      "Chunk Size: 500\n",
      "\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "Iteration: 9\\Accuracy: 0.5980000000000001\n",
      "Precision: 0.5241612835050995\n",
      "\n",
      "Chunk Size: 1000\n",
      "\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "3 1\n",
      "Iteration: 9\\Accuracy: 0.642\n",
      "Precision: 0.5732598401009589\n",
      "\n",
      "Chunk Size: 5000\n",
      "\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 1\n",
      "4 0\n",
      "Iteration: 9\\Accuracy: 0.696\n",
      "Precision: 0.48555465324866603\n",
      "\n",
      "Chunk Size: 10000\n",
      "\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "Iteration: 9\\Accuracy: 0.6611999999999999\n",
      "Precision: 0.5275562432303633\n",
      "\n",
      "Chunk Size: 39644\n",
      "\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "4 0\n",
      "Iteration: 9\\Accuracy: 0.6310414219781311\n",
      "Precision: 0.40256987234603486\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y, \"The Sum Dataset(with noise)\", LinearRegression, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\",  LinearRegression, True) \n",
    "# model(news_reg_X, news_reg_Y, \"News dataset\", LinearRegression, True)\n",
    "\n",
    "\n",
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y,\"The Sum Dataset(with noise)\",  RandomForestRegressor, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\", RandomForestRegressor, True) \n",
    "# model(news_reg_X, news_reg_Y, \"News dataset\", RandomForestRegressor, True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# model(sumdata_noise_classif_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", LogisticRegression, False)\n",
    "#model(sumdata_classif_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", LogisticRegression, False)\n",
    "#model(news_reg_X, news_classif_Y, \"News dataset\", LogisticRegression, False)\n",
    " \n",
    "# from sklearn.svm import SVC\n",
    "# model(sumdata_noise_classi_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", SVC, False)\n",
    "# model(sumdata_classi_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", SVC, False)\n",
    "# model(housing_price_classi_X, housing_price_classif_Y, \"Housing Dataset\", SVC, False)\n",
    "# model(titanic_classification_X, titanic_classification_y, \"Titanic Dataset\", SVC, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
