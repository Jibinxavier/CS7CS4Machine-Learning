{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from helper import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE THE DATASETS ARE CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![caption](files/requirements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AABABUTdx7YqCeBquA1Ky7z8a/The%20SUM%20dataset?dl=1#\"\n",
    "news_url  = \"https://www.dropbox.com/sh/euppz607r6gsen2/AACq4aMWDOIw2I_SSGqJ-r2Oa/Online%20News%20Popularity%20(Mashable%20News)?dl=1\"\n",
    "housing_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AAD6JGlvG5XADIjg9SCojvpya/House%20Sales%20in%20King%20County%2C%20USA?dl=1&preview=kc_house_data.csv\"\n",
    "all_urls = [sumdata_url,news_url,housing_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data(all_urls) # retrieves the data if there is NO data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_noise_path = \"data/with noise/The SUM dataset, with noise.csv\"\n",
    "sumdata_path = \"data/without noise/The SUM dataset, without noise.csv\" \n",
    "housing_price_path =\"data/kc_house_data.csv\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sum_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Noisy Target</th>\n",
       "      <th>Noisy Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>66957</td>\n",
       "      <td>74432</td>\n",
       "      <td>96087</td>\n",
       "      <td>103120</td>\n",
       "      <td>64272</td>\n",
       "      <td>150633</td>\n",
       "      <td>181787</td>\n",
       "      <td>180349</td>\n",
       "      <td>216912</td>\n",
       "      <td>304071</td>\n",
       "      <td>1434819</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>96030</td>\n",
       "      <td>86875</td>\n",
       "      <td>108299</td>\n",
       "      <td>148025</td>\n",
       "      <td>16965</td>\n",
       "      <td>253819</td>\n",
       "      <td>258672</td>\n",
       "      <td>268851</td>\n",
       "      <td>404599</td>\n",
       "      <td>543092</td>\n",
       "      <td>2148748</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 1  Feature 2  Feature 3  Feature 4  Feature 5 (meaningless)  \\\n",
       "0      66957      74432      96087     103120                    64272   \n",
       "1      96030      86875     108299     148025                    16965   \n",
       "\n",
       "   Feature 6  Feature 7  Feature 8  Feature 9  Feature 10  Noisy Target  \\\n",
       "0     150633     181787     180349     216912      304071       1434819   \n",
       "1     253819     258672     268851     404599      543092       2148748   \n",
       "\n",
       "  Noisy Target Class  \n",
       "0  Very Large Number  \n",
       "1  Very Large Number  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata_noise = pd.read_csv(sumdata_noise_path, delimiter=\";\")\n",
    "\n",
    "# Remove 'Instance' as it simply represents the row number\n",
    "sumdata_noise = sumdata_noise.drop('Instance', axis = 1)\n",
    "sumdata_noise.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sum_noise dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_noise_reg_Y = sumdata_noise['Noisy Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as regression target\n",
    "sumdata_noise_classif_Y = pd.get_dummies(sumdata_noise['Noisy Target Class']).iloc[:, 0]\n",
    "sumdata_noise_classif_Y = sumdata_noise_classif_Y.values.astype(int).reshape(-1,1)\n",
    "\n",
    "# Use rest columns as explananatory variables\n",
    "# We can simply use the same features for both as Noisy Target and Noisy Target Class are representing the samething\n",
    "sumdata_noise_reg_X = sumdata_noise.iloc[:, 0:-2].values\n",
    "sumdata_noise_classif_X = sumdata_noise.iloc[:, 0:-2].values\n",
    "\n",
    "# Apply Feature Scaling for the classification variable\n",
    "# As we are using KNN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scX = StandardScaler()\n",
    "sumdata_noise_classif_X = scX.fit_transform(sumdata_noise_classif_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sumdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57326</td>\n",
       "      <td>68791</td>\n",
       "      <td>82549</td>\n",
       "      <td>99059</td>\n",
       "      <td>72624</td>\n",
       "      <td>142645</td>\n",
       "      <td>171174</td>\n",
       "      <td>205409</td>\n",
       "      <td>246491</td>\n",
       "      <td>295789</td>\n",
       "      <td>1073444</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87859</td>\n",
       "      <td>105431</td>\n",
       "      <td>126517</td>\n",
       "      <td>151820</td>\n",
       "      <td>19982</td>\n",
       "      <td>218621</td>\n",
       "      <td>262345</td>\n",
       "      <td>314814</td>\n",
       "      <td>377777</td>\n",
       "      <td>453332</td>\n",
       "      <td>1645184</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 1  Feature 2  Feature 3  Feature 4  Feature 5 (meaningless)  \\\n",
       "0      57326      68791      82549      99059                    72624   \n",
       "1      87859     105431     126517     151820                    19982   \n",
       "\n",
       "   Feature 6  Feature 7  Feature 8  Feature 9  Feature 10   Target  \\\n",
       "0     142645     171174     205409     246491      295789  1073444   \n",
       "1     218621     262345     314814     377777      453332  1645184   \n",
       "\n",
       "        Target Class  \n",
       "0  Very Large Number  \n",
       "1  Very Large Number  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata = pd.read_csv(sumdata_path, delimiter=\";\")\n",
    "\n",
    "# Remove 'Instance' as it simply represents the row number\n",
    "sumdata = sumdata.drop('Instance', axis = 1)\n",
    "sumdata.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sumdata dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest of the columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiyim/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_reg_Y = sumdata['Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as classification target\n",
    "sumdata_classif_Y = pd.get_dummies(sumdata['Target Class']).iloc[:, 0]\n",
    "sumdata_classif_Y = sumdata_classif_Y.values.astype(int).reshape(-1,1)\n",
    "\n",
    "# Use rest columns as explananatory variables\n",
    "# We can simply use the same features for both as Target and Target Class are representing the same thing\n",
    "sumdata_classif_X = sumdata.iloc[:, 0:-2].values\n",
    "sumdata_reg_X = sumdata.iloc[:, 0:-2].values\n",
    "\n",
    "# Apply Feature Scaling for the classification variable\n",
    "# As we are using KNN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "sumdata_classif_X = scX.fit_transform(sumdata_classif_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## News dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "# online_news =  get_news_dataset()\n",
    "\n",
    "news = get_news_dataset()\n",
    "print(len(news))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>timedelta</th>\n",
       "      <th>n_tokens_title</th>\n",
       "      <th>n_tokens_content</th>\n",
       "      <th>n_unique_tokens</th>\n",
       "      <th>n_non_stop_words</th>\n",
       "      <th>n_non_stop_unique_tokens</th>\n",
       "      <th>num_hrefs</th>\n",
       "      <th>num_self_hrefs</th>\n",
       "      <th>num_imgs</th>\n",
       "      <th>num_videos</th>\n",
       "      <th>average_token_length</th>\n",
       "      <th>num_keywords</th>\n",
       "      <th>data_channel_is_lifestyle</th>\n",
       "      <th>data_channel_is_entertainment</th>\n",
       "      <th>data_channel_is_bus</th>\n",
       "      <th>data_channel_is_socmed</th>\n",
       "      <th>data_channel_is_tech</th>\n",
       "      <th>data_channel_is_world</th>\n",
       "      <th>kw_min_min</th>\n",
       "      <th>kw_max_min</th>\n",
       "      <th>kw_avg_min</th>\n",
       "      <th>kw_min_max</th>\n",
       "      <th>kw_max_max</th>\n",
       "      <th>kw_avg_max</th>\n",
       "      <th>kw_min_avg</th>\n",
       "      <th>kw_max_avg</th>\n",
       "      <th>kw_avg_avg</th>\n",
       "      <th>self_reference_min_shares</th>\n",
       "      <th>self_reference_max_shares</th>\n",
       "      <th>self_reference_avg_sharess</th>\n",
       "      <th>weekday_is_monday</th>\n",
       "      <th>weekday_is_tuesday</th>\n",
       "      <th>weekday_is_wednesday</th>\n",
       "      <th>weekday_is_thursday</th>\n",
       "      <th>weekday_is_friday</th>\n",
       "      <th>weekday_is_saturday</th>\n",
       "      <th>weekday_is_sunday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>LDA_00</th>\n",
       "      <th>LDA_01</th>\n",
       "      <th>LDA_02</th>\n",
       "      <th>LDA_03</th>\n",
       "      <th>LDA_04</th>\n",
       "      <th>global_subjectivity</th>\n",
       "      <th>global_sentiment_polarity</th>\n",
       "      <th>global_rate_positive_words</th>\n",
       "      <th>global_rate_negative_words</th>\n",
       "      <th>rate_positive_words</th>\n",
       "      <th>rate_negative_words</th>\n",
       "      <th>avg_positive_polarity</th>\n",
       "      <th>min_positive_polarity</th>\n",
       "      <th>max_positive_polarity</th>\n",
       "      <th>avg_negative_polarity</th>\n",
       "      <th>min_negative_polarity</th>\n",
       "      <th>max_negative_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>title_sentiment_polarity</th>\n",
       "      <th>abs_title_subjectivity</th>\n",
       "      <th>abs_title_sentiment_polarity</th>\n",
       "      <th>shares</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://mashable.com/2013/01/07/amazon-instant-...</td>\n",
       "      <td>731.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>219.0</td>\n",
       "      <td>0.663594</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.815385</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.680365</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>496.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.500331</td>\n",
       "      <td>0.378279</td>\n",
       "      <td>0.040005</td>\n",
       "      <td>0.041263</td>\n",
       "      <td>0.040123</td>\n",
       "      <td>0.521617</td>\n",
       "      <td>0.092562</td>\n",
       "      <td>0.045662</td>\n",
       "      <td>0.013699</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.378636</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1875</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1875</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url   timedelta  \\\n",
       "0  http://mashable.com/2013/01/07/amazon-instant-...       731.0   \n",
       "\n",
       "    n_tokens_title   n_tokens_content   n_unique_tokens   n_non_stop_words  \\\n",
       "0             12.0              219.0          0.663594                1.0   \n",
       "\n",
       "    n_non_stop_unique_tokens   num_hrefs   num_self_hrefs   num_imgs  \\\n",
       "0                   0.815385         4.0              2.0        1.0   \n",
       "\n",
       "    num_videos   average_token_length   num_keywords  \\\n",
       "0          0.0               4.680365            5.0   \n",
       "\n",
       "    data_channel_is_lifestyle   data_channel_is_entertainment  \\\n",
       "0                         0.0                             1.0   \n",
       "\n",
       "    data_channel_is_bus   data_channel_is_socmed   data_channel_is_tech  \\\n",
       "0                   0.0                      0.0                    0.0   \n",
       "\n",
       "    data_channel_is_world   kw_min_min   kw_max_min   kw_avg_min   kw_min_max  \\\n",
       "0                     0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    kw_max_max   kw_avg_max   kw_min_avg   kw_max_avg   kw_avg_avg  \\\n",
       "0          0.0          0.0          0.0          0.0          0.0   \n",
       "\n",
       "    self_reference_min_shares   self_reference_max_shares  \\\n",
       "0                       496.0                       496.0   \n",
       "\n",
       "    self_reference_avg_sharess   weekday_is_monday   weekday_is_tuesday  \\\n",
       "0                        496.0                 1.0                  0.0   \n",
       "\n",
       "    weekday_is_wednesday   weekday_is_thursday   weekday_is_friday  \\\n",
       "0                    0.0                   0.0                 0.0   \n",
       "\n",
       "    weekday_is_saturday   weekday_is_sunday   is_weekend    LDA_00    LDA_01  \\\n",
       "0                   0.0                 0.0          0.0  0.500331  0.378279   \n",
       "\n",
       "     LDA_02    LDA_03    LDA_04   global_subjectivity  \\\n",
       "0  0.040005  0.041263  0.040123              0.521617   \n",
       "\n",
       "    global_sentiment_polarity   global_rate_positive_words  \\\n",
       "0                    0.092562                     0.045662   \n",
       "\n",
       "    global_rate_negative_words   rate_positive_words   rate_negative_words  \\\n",
       "0                     0.013699              0.769231              0.230769   \n",
       "\n",
       "    avg_positive_polarity   min_positive_polarity   max_positive_polarity  \\\n",
       "0                0.378636                     0.1                     0.7   \n",
       "\n",
       "    avg_negative_polarity   min_negative_polarity   max_negative_polarity  \\\n",
       "0                   -0.35                    -0.6                    -0.2   \n",
       "\n",
       "    title_subjectivity   title_sentiment_polarity   abs_title_subjectivity  \\\n",
       "0                  0.5                    -0.1875                      0.0   \n",
       "\n",
       "    abs_title_sentiment_polarity   shares  \n",
       "0                         0.1875      593  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features  that will be selected BE CAREFUL ALL COLUMN NAMES HAVE SPACE AT THE START"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns= [ ' timedelta', ' n_tokens_title', ' n_tokens_content',\n",
    "       ' n_unique_tokens', ' n_non_stop_words', ' n_non_stop_unique_tokens',\n",
    "       ' num_hrefs', ' num_self_hrefs', ' num_imgs', ' num_videos',\n",
    "       ' average_token_length', ' num_keywords', ' data_channel_is_lifestyle',\n",
    "       ' data_channel_is_entertainment', ' data_channel_is_bus',\n",
    "       ' data_channel_is_socmed', ' data_channel_is_tech',\n",
    "       ' data_channel_is_world', ' kw_min_min', ' kw_max_min', ' kw_avg_min',\n",
    "       ' kw_min_max', ' kw_max_max', ' kw_avg_max', ' kw_min_avg',\n",
    "       ' kw_max_avg', ' kw_avg_avg', ' self_reference_min_shares',\n",
    "       ' self_reference_max_shares', ' self_reference_avg_sharess', ' global_subjectivity', ' global_rate_positive_words',\n",
    "       ' global_rate_negative_words', ' rate_positive_words',\n",
    "       ' rate_negative_words', ' avg_positive_polarity',\n",
    "       ' min_positive_polarity', ' max_positive_polarity',' title_subjectivity', ' abs_title_subjectivity',\n",
    "       ' abs_title_sentiment_polarity', ' shares']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classes\n",
    "- 0 - 140550       class 1\n",
    "- 140551 - 281101  class 2\n",
    "- 281102 - 421651  class 3\n",
    "- 421652 - 562202  class 4\n",
    "- 562203 - 702752  class 5\n",
    "- 702752 -         class 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_classes(row):\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "  \n",
    "\n",
    "    if(row[' shares'] <= 140550):\n",
    "        \n",
    "        return 1\n",
    "    elif( row[' shares'] > 140550 and row[' shares'] <= 281101  ):\n",
    "        \n",
    "        return 2\n",
    "    elif( row[' shares'] > 281101 and row[' shares'] <= 421651  ):\n",
    "         \n",
    "        return 3\n",
    "    elif( row[' shares'] > 421651 and row[' shares'] <= 562202  ):\n",
    "         \n",
    "        return 4\n",
    "\n",
    "    elif( row[' shares'] > 562202 and row[' shares'] <= 702752  ):\n",
    "         \n",
    "        return 5\n",
    "    else:\n",
    "        \n",
    "        return 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39644\n"
     ]
    }
   ],
   "source": [
    "news = news[columns]\n",
    "print(len(news))\n",
    "# Classification  the target values are going t\n",
    "news['label'] = news.apply(create_classes, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_reg_Y = news[' shares'].values.reshape(-1, 1)\n",
    "\n",
    "news_reg_X = news.iloc[:, 0:-2] # the last 2 columns are the target values \n",
    "news_classif_Y = news['label'].values.reshape(-1, 1)\n",
    "\n",
    "\n",
    "# from sklearn import preprocessing\n",
    "# le = preprocessing.LabelEncoder()\n",
    "\n",
    "# news_classif_Y = le.fit_transform(news_classif_Y)\n",
    "max(news_classif_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits Algorithms to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000,\n",
    "1000000, 5000000, 10000000, 50000000, 100000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from math import sqrt\n",
    "def root_mean_square_error(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model( X, y, dataset_name, algorithm, isReg): \n",
    "    \n",
    "    print (\"Algorithm: {}\\nDataset: {}\\n\".format( algorithm.__name__, dataset_name))\n",
    "    for chunk in data_chunks:\n",
    "        \n",
    "        # if chunk is greater than the no. of examples quite from the chunking\n",
    "        if chunk > X.shape[0]: \n",
    "            chunk = X.shape[0]\n",
    "        \n",
    "        print (\"Chunk Size: {}\\n\".format(chunk))\n",
    "        \n",
    "        # generate the chunk file\n",
    "        current_X = X[0:chunk]\n",
    "        current_y = y[0:chunk]\n",
    "        \n",
    "        kFoldModelling(current_X, current_y, 10, algorithm, isReg)\n",
    "        \n",
    "        if chunk == X.shape[0]:\n",
    "            break\n",
    "              \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFoldModelling (X, y, kfolds, algorithm, isReg):\n",
    "    \n",
    "    kf = KFold(n_splits=kfolds)\n",
    "    rmse = np.zeros((10,1))\n",
    "    mae = np.zeros((10,1))\n",
    "    accuracy = np.zeros((10,1))\n",
    "    precision = np.zeros((10,1))\n",
    "    X = X.as_matrix()\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # fit the model to the datset\n",
    "        lm = algorithm()\n",
    "        lm.fit(X_train, y_train)\n",
    "        \n",
    "        if isReg:     \n",
    "            rmse[i] = root_mean_square_error(y_test, lm.predict(X_test))  # RMSE https://www.kaggle.com/wiki/RootMeanSquaredError\n",
    "            mae[i] = mean_absolute_error(y_test, lm.predict(X_test))\n",
    "        else:\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy[i] = accuracy_score(y_test, lm.predict(X_test))\n",
    "            precision[i] = precision_score(y_test, lm.predict(X_test))\n",
    "        # print the result, we will need to have method that genereates the result csv file required.\n",
    "    \n",
    "    if isReg:     \n",
    "        print (\"Iteration: {}\\nRMSE: {}\\nMAE: {}\\n\".format( i, rmse.mean(), mae.mean()))\n",
    "    else:\n",
    "        print (\"Iteration: {}\\Accuracy: {}\\nPrecision: {}\\n\".format( i, accuracy.mean(), precision.mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fits Regression Algorithms to datasets\n",
    "\n",
    "    - Linear Regression\n",
    "    - Random Forest Regression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(news_classif_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y, \"The Sum Dataset(with noise)\", LinearRegression, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\",  LinearRegression, True)\n",
    "# model(housing_price_reg_X, housing_price_reg_Y, \"Housing Dataset\",  LinearRegression, True)\n",
    "# model(titanic_regression_X, titanic_regression_Y, \"Titanic Dataset\", LinearRegression, True)\n",
    "#model(news_reg_X, news_reg_Y, \"News dataset\", LinearRegression, True)\n",
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y,\"The Sum Dataset(with noise)\",  RandomForestRegressor, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\", RandomForestRegressor, True)\n",
    "# model(housing_price_reg_X, housing_price_reg_Y, \"Housing Dataset\", RandomForestRegressor, True)\n",
    "# model(titanic_regression_X, titanic_regression_Y, \"Titanic Dataset\", RandomForestRegressor, True)\n",
    " \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# model(sumdata_noise_classif_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", LogisticRegression, False)\n",
    "# model(sumdata_classif_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", LogisticRegression, False)\n",
    "model(news_reg_X, news_classif_Y, \"News dataset\", LogisticRegression, False)\n",
    "news_classif_Y.min()\n",
    "# model(titanic_classification_X, titanic_classification_y, \"Titanic Dataset\", LogisticRegression, False)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# model(sumdata_noise_classi_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", SVC, False)\n",
    "# model(sumdata_classi_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", SVC, False)\n",
    "# model(housing_price_classi_X, housing_price_classif_Y, \"Housing Dataset\", SVC, False)\n",
    "# model(titanic_classification_X, titanic_classification_y, \"Titanic Dataset\", SVC, False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
