{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold\n",
    "import pandas as pd\n",
    "from helper import *\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKE SURE THE DATASETS ARE CORRECT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ![caption](files/requirements.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AABABUTdx7YqCeBquA1Ky7z8a/The%20SUM%20dataset?dl=1#\"\n",
    "news_url  = \"https://www.dropbox.com/sh/euppz607r6gsen2/AACq4aMWDOIw2I_SSGqJ-r2Oa/Online%20News%20Popularity%20(Mashable%20News)?dl=1\"\n",
    "housing_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AAD6JGlvG5XADIjg9SCojvpya/House%20Sales%20in%20King%20County%2C%20USA?dl=1&preview=kc_house_data.csv\"\n",
    "all_urls = [census_url,news_url,housing_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(all_urls) # retrieves the data if there is NO data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_noise_path = \"data/with noise/The SUM dataset, with noise.csv\"\n",
    "sumdata_path = \"data/without noise/The SUM dataset, without noise.csv\" \n",
    "housing_price_path =\"data/kc_house_data.csv\"\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sum_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless but please still use it)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Noisy Target</th>\n",
       "      <th>Noisy Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62485</td>\n",
       "      <td>58472</td>\n",
       "      <td>84200</td>\n",
       "      <td>86181</td>\n",
       "      <td>75529</td>\n",
       "      <td>136939</td>\n",
       "      <td>150633</td>\n",
       "      <td>230058</td>\n",
       "      <td>246491</td>\n",
       "      <td>257336</td>\n",
       "      <td>1352179</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>75559</td>\n",
       "      <td>119137</td>\n",
       "      <td>146760</td>\n",
       "      <td>139674</td>\n",
       "      <td>19582</td>\n",
       "      <td>177083</td>\n",
       "      <td>217746</td>\n",
       "      <td>321110</td>\n",
       "      <td>434444</td>\n",
       "      <td>516798</td>\n",
       "      <td>1976446</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0      62485      58472      84200      86181   \n",
       "1      75559     119137     146760     139674   \n",
       "\n",
       "   Feature 5 (meaningless but please still use it)  Feature 6  Feature 7  \\\n",
       "0                                            75529     136939     150633   \n",
       "1                                            19582     177083     217746   \n",
       "\n",
       "   Feature 8  Feature 9  Feature 10  Noisy Target Noisy Target Class  \n",
       "0     230058     246491      257336       1352179  Very Large Number  \n",
       "1     321110     434444      516798       1976446  Very Large Number  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata_noise = pd.read_csv(sumdata_noise_path, delimiter=\";\")\n",
    "\n",
    "# Remove 'Instance' as it simply represents the row number\n",
    "sumdata_noise = sumdata_noise.drop('Instance', axis = 1)\n",
    "sumdata_noise.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sum_noise dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_noise_reg_Y = sumdata_noise['Noisy Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as regression target\n",
    "sumdata_noise_classif_Y = pd.get_dummies(sumdata_noise['Noisy Target Class']).iloc[:, 0]\n",
    "sumdata_noise_classif_Y = sumdata_noise_classif_Y.values.astype(int).reshape(-1,1)\n",
    "\n",
    "# Use rest columns as explananatory variables\n",
    "# We can simply use the same features for both as Noisy Target and Noisy Target Class are representing the samething\n",
    "sumdata_noise_reg_X = sumdata_noise.iloc[:, 0:-2].values\n",
    "sumdata_noise_classif_X = sumdata_noise.iloc[:, 0:-2].values\n",
    "\n",
    "# Apply Feature Scaling for the classification variable\n",
    "# As we are using KNN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scX = StandardScaler()\n",
    "sumdata_noise_classif_X = scX.fit_transform(sumdata_noise_classif_X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sumdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless but please still use it)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Target</th>\n",
       "      <th>Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>57326</td>\n",
       "      <td>68791</td>\n",
       "      <td>82549</td>\n",
       "      <td>99059</td>\n",
       "      <td>72624</td>\n",
       "      <td>142645</td>\n",
       "      <td>171174</td>\n",
       "      <td>205409</td>\n",
       "      <td>246491</td>\n",
       "      <td>295789</td>\n",
       "      <td>1369233</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>87859</td>\n",
       "      <td>105431</td>\n",
       "      <td>126517</td>\n",
       "      <td>151820</td>\n",
       "      <td>19982</td>\n",
       "      <td>218621</td>\n",
       "      <td>262345</td>\n",
       "      <td>314814</td>\n",
       "      <td>377777</td>\n",
       "      <td>453332</td>\n",
       "      <td>2098516</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0      57326      68791      82549      99059   \n",
       "1      87859     105431     126517     151820   \n",
       "\n",
       "   Feature 5 (meaningless but please still use it)  Feature 6  Feature 7  \\\n",
       "0                                            72624     142645     171174   \n",
       "1                                            19982     218621     262345   \n",
       "\n",
       "   Feature 8  Feature 9  Feature 10   Target       Target Class  \n",
       "0     205409     246491      295789  1369233  Very Large Number  \n",
       "1     314814     377777      453332  2098516  Very Large Number  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata = pd.read_csv(sumdata_path, delimiter=\";\")\n",
    "\n",
    "# Remove 'Instance' as it simply represents the row number\n",
    "sumdata = sumdata.drop('Instance', axis = 1)\n",
    "sumdata.head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sumdata dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest of the columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:429: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, _DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "# Use 'Nosiy Target' as regression target\n",
    "sumdata_reg_Y = sumdata['Target'].values.reshape(-1, 1)\n",
    "\n",
    "# Use 'Nosiy Target Class' Large Number as classification target\n",
    "sumdata_classif_Y = pd.get_dummies(sumdata['Target Class']).iloc[:, 0]\n",
    "sumdata_classif_Y = sumdata_classif_Y.values.astype(int).reshape(-1,1)\n",
    "\n",
    "# Use rest columns as explananatory variables\n",
    "# We can simply use the same features for both as Target and Target Class are representing the same thing\n",
    "sumdata_classif_X = sumdata.iloc[:, 0:-2].values\n",
    "sumdata_reg_X = sumdata.iloc[:, 0:-2].values\n",
    "\n",
    "# Apply Feature Scaling for the classification variable\n",
    "# As we are using KNN \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "sumdata_classif_X = scX.fit_transform(sumdata_classif_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load census dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Load House Price dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_price = pd.read_csv(housing_price_path)\n",
    "\n",
    "# Remove 'Instance' as it simply represents the row number\n",
    "housing_price = housing_price.drop('id', axis = 1)\n",
    "housing_price = housing_price.assign(city=\"\") # adding city\n",
    "housing_price= housing_price.head(n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sales dataset\n",
    "\n",
    "- Remove 'Id' as it simply represents the row number\n",
    "- Remove \"data\" which is not important\n",
    "- Use 'SalePrice' as regression target\n",
    "- Use 'SaleCondition' as classification target\n",
    "- For explananatory variables, use the following numerical variable and categorical variables\n",
    "    - Numerical\n",
    "        - LotFrontage\n",
    "        - OverallQual\n",
    "        - OverallCond\n",
    "        - YearBuilt\n",
    "        - YearRemodAdd\n",
    "        - TotalBsmtSF\n",
    "        - GrLivArea\n",
    "        - FullBath\n",
    "        - TotRmsAbvGrd\n",
    "        - GarageYrBlt\n",
    "        - GarageCars\n",
    "        - GarageArea\n",
    "        - LotArea\n",
    "        - 1stFlrSF\n",
    "        - 2ndFlrSF\n",
    "    - Categorical\n",
    "        - YearSold\n",
    "        - Neighborhood\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>20140512T000000</td>\n",
       "      <td>1225000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.50</td>\n",
       "      <td>5420</td>\n",
       "      <td>101930</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3890</td>\n",
       "      <td>1530</td>\n",
       "      <td>2001</td>\n",
       "      <td>0</td>\n",
       "      <td>98053</td>\n",
       "      <td>47.6561</td>\n",
       "      <td>-122.005</td>\n",
       "      <td>4760</td>\n",
       "      <td>101930</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20140627T000000</td>\n",
       "      <td>257500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1715</td>\n",
       "      <td>6819</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1715</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98003</td>\n",
       "      <td>47.3097</td>\n",
       "      <td>-122.327</td>\n",
       "      <td>2238</td>\n",
       "      <td>6819</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>20150115T000000</td>\n",
       "      <td>291850.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.50</td>\n",
       "      <td>1060</td>\n",
       "      <td>9711</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1060</td>\n",
       "      <td>0</td>\n",
       "      <td>1963</td>\n",
       "      <td>0</td>\n",
       "      <td>98198</td>\n",
       "      <td>47.4095</td>\n",
       "      <td>-122.315</td>\n",
       "      <td>1650</td>\n",
       "      <td>9711</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>20150415T000000</td>\n",
       "      <td>229500.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1780</td>\n",
       "      <td>7470</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>730</td>\n",
       "      <td>1960</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5123</td>\n",
       "      <td>-122.337</td>\n",
       "      <td>1780</td>\n",
       "      <td>8113</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>20150312T000000</td>\n",
       "      <td>323000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1890</td>\n",
       "      <td>6560</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1890</td>\n",
       "      <td>0</td>\n",
       "      <td>2003</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3684</td>\n",
       "      <td>-122.031</td>\n",
       "      <td>2390</td>\n",
       "      <td>7570</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              date      price  bedrooms  bathrooms  sqft_living  sqft_lot  \\\n",
       "0  20141013T000000   221900.0         3       1.00         1180      5650   \n",
       "1  20141209T000000   538000.0         3       2.25         2570      7242   \n",
       "2  20150225T000000   180000.0         2       1.00          770     10000   \n",
       "3  20141209T000000   604000.0         4       3.00         1960      5000   \n",
       "4  20150218T000000   510000.0         3       2.00         1680      8080   \n",
       "5  20140512T000000  1225000.0         4       4.50         5420    101930   \n",
       "6  20140627T000000   257500.0         3       2.25         1715      6819   \n",
       "7  20150115T000000   291850.0         3       1.50         1060      9711   \n",
       "8  20150415T000000   229500.0         3       1.00         1780      7470   \n",
       "9  20150312T000000   323000.0         3       2.50         1890      6560   \n",
       "\n",
       "   floors  waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "0     1.0           0     0          3      7        1180              0   \n",
       "1     2.0           0     0          3      7        2170            400   \n",
       "2     1.0           0     0          3      6         770              0   \n",
       "3     1.0           0     0          5      7        1050            910   \n",
       "4     1.0           0     0          3      8        1680              0   \n",
       "5     1.0           0     0          3     11        3890           1530   \n",
       "6     2.0           0     0          3      7        1715              0   \n",
       "7     1.0           0     0          3      7        1060              0   \n",
       "8     1.0           0     0          3      7        1050            730   \n",
       "9     2.0           0     0          3      7        1890              0   \n",
       "\n",
       "   yr_built  yr_renovated  zipcode      lat     long  sqft_living15  \\\n",
       "0      1955             0    98178  47.5112 -122.257           1340   \n",
       "1      1951          1991    98125  47.7210 -122.319           1690   \n",
       "2      1933             0    98028  47.7379 -122.233           2720   \n",
       "3      1965             0    98136  47.5208 -122.393           1360   \n",
       "4      1987             0    98074  47.6168 -122.045           1800   \n",
       "5      2001             0    98053  47.6561 -122.005           4760   \n",
       "6      1995             0    98003  47.3097 -122.327           2238   \n",
       "7      1963             0    98198  47.4095 -122.315           1650   \n",
       "8      1960             0    98146  47.5123 -122.337           1780   \n",
       "9      2003             0    98038  47.3684 -122.031           2390   \n",
       "\n",
       "   sqft_lot15 city  \n",
       "0        5650       \n",
       "1        7639       \n",
       "2        8062       \n",
       "3        5000       \n",
       "4        7503       \n",
       "5      101930       \n",
       "6        6819       \n",
       "7        9711       \n",
       "8        8113       \n",
       "9        7570       "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## filter unused categorical columns and drop NaN, we only want to keep 'neighborhood' and 'housestyle' \n",
    "## in our explanantory vairbale\n",
    "\n",
    "# get all the numerical data that is needed\n",
    "explanatory_numeric = ['OverallQual', 'LotFrontage', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'TotalBsmtSF', 'GrLivArea',\n",
    "                      'FullBath', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'LotArea', '1stFlrSF', '2ndFlrSF']\n",
    "\n",
    "housing_price[explanatory_numeric[0]]\n",
    "filtered_table = housing_price[explanatory_numeric[0]]\n",
    "filtered_table\n",
    "\n",
    "for c in explanatory_numeric[1:]:\n",
    "    filtered_table = pd.concat([filtered_table, housing_price[c]], axis=1)\n",
    "filtered_table\n",
    "\n",
    "# get the two categorical variable that we wants to use, convert it to dummy variable and avoid dummy variable trap\n",
    "filtered_table = pd.concat([filtered_table, pd.get_dummies(housing_price['Neighborhood']).iloc[:, :-1]], axis=1)\n",
    "filtered_table = pd.concat([filtered_table, pd.get_dummies(housing_price['YrSold']).iloc[:, :-1]], axis=1)\n",
    "\n",
    "# drop NaN\n",
    "filtered_table = filtered_table.dropna()\n",
    "filtered_table \n",
    "\n",
    "# get explanatory variable\n",
    "housing_price_reg_X = filtered_table[:].values\n",
    "housing_price_classif_X = filtered_table[:].values\n",
    "\n",
    "# get the regression target and classification target\n",
    "# I am using sales price as both regression and classification target\n",
    "# sale_price_classi = SalePrice<Mean, SalePrice>=Mean\n",
    "housing_price_reg_Y = housing_price['SalePrice'].values\n",
    "housing_price_classif_Y = np.zeros(housing_price_reg_Y.shape)\n",
    "house_price_mean = housing_price_reg_Y.mean()\n",
    "\n",
    "for i, price in enumerate(housing_price_reg_Y):\n",
    "    if price >= house_price_mean:\n",
    "        housing_price_classi_Y[i] = 1\n",
    "    else:\n",
    "        housing_price_classi_Y[i] = 0\n",
    "\n",
    "# Apply Feature Scaling to the numeric variables and regression target \n",
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "housing_price_classif_X = scX.fit_transform(housing_price_classif_X)\n",
    "housing_price_classif_Y = scY.fit_transform(housing_price_classif_Y.reshape(-1, 1))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fits Algorithms to datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000,\n",
    "1000000, 5000000, 10000000, 50000000, 100000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score\n",
    "from math import sqrt\n",
    "def root_mean_square_error(y_actual, y_predicted):\n",
    "    return sqrt(mean_squared_error(y_actual, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model( X, y, dataset_name, algorithm, isReg): \n",
    "    \n",
    "    print (\"Algorithm: {}\\nDataset: {}\\n\".format( algorithm.__name__, dataset_name))\n",
    "    for chunk in data_chunks:\n",
    "        \n",
    "        # if chunk is greater than the no. of examples quite from the chunking\n",
    "        if chunk > X.shape[0]: \n",
    "            chunk = X.shape[0]\n",
    "        \n",
    "        print (\"Chunk Size: {}\\n\".format(chunk))\n",
    "        \n",
    "        # generate the chunk file\n",
    "        current_X = X[0:chunk]\n",
    "        current_y = y[0:chunk]\n",
    "        \n",
    "        kFoldModelling(current_X, current_y, 10, algorithm, isReg)\n",
    "        \n",
    "        if chunk == X.shape[0]:\n",
    "            break\n",
    "              \n",
    "         \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kFoldModelling (X, y, kfolds, algorithm, isReg):\n",
    "    \n",
    "    kf = KFold(n_splits=kfolds)\n",
    "    rmse = np.zeros((10,1))\n",
    "    mae = np.zeros((10,1))\n",
    "    accuracy = np.zeros((10,1))\n",
    "    precision = np.zeros((10,1))\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        # fit the model to the datset\n",
    "        lm = algorithm()\n",
    "        lm.fit(X_train, y_train)\n",
    "        \n",
    "        if isReg:     \n",
    "            rmse[i] = root_mean_square_error(y_test, lm.predict(X_test))  # RMSE https://www.kaggle.com/wiki/RootMeanSquaredError\n",
    "            mae[i] = mean_absolute_error(y_test, lm.predict(X_test))\n",
    "        else:\n",
    "            from sklearn.metrics import accuracy_score\n",
    "            accuracy[i] = accuracy_score(y_test, lm.predict(X_test))\n",
    "            precision[i] = precision_score(y_test, lm.predict(X_test))\n",
    "        # print the result, we will need to have method that genereates the result csv file required.\n",
    "    \n",
    "    if isReg:     \n",
    "        print (\"Iteration: {}\\nRMSE: {}\\nMAE: {}\\n\".format( i, rmse.mean(), mae.mean()))\n",
    "    else:\n",
    "        print (\"Iteration: {}\\Accuracy: {}\\nPrecision: {}\\n\".format( i, accuracy.mean(), precision.mean()))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fits Regression Algorithms to datasets\n",
    "\n",
    "    - Linear Regression\n",
    "    - Random Forest Regression\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       ..., \n",
       "       [ 0.],\n",
       "       [ 0.],\n",
       "       [ 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y, \"The Sum Dataset(with noise)\", LinearRegression, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\",  LinearRegression, True)\n",
    "# model(housing_price_reg_X, housing_price_reg_Y, \"Housing Dataset\",  LinearRegression, True)\n",
    "# model(titanic_regression_X, titanic_regression_Y, \"Titanic Dataset\", LinearRegression, True)\n",
    "\n",
    "# model(sumdata_noise_reg_X, sumdata_noise_reg_Y,\"The Sum Dataset(with noise)\",  RandomForestRegressor, True)\n",
    "# model(sumdata_reg_X, sumdata_reg_Y, \"The Sum Dataset(without noise)\", RandomForestRegressor, True)\n",
    "# model(housing_price_reg_X, housing_price_reg_Y, \"Housing Dataset\", RandomForestRegressor, True)\n",
    "# model(titanic_regression_X, titanic_regression_Y, \"Titanic Dataset\", RandomForestRegressor, True)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# model(sumdata_noise_classif_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", LogisticRegression, False)\n",
    "# model(sumdata_classif_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", LogisticRegression, False)\n",
    "# model(housing_price_classif_X, housing_price_classif_Y, \"Housing Dataset\", LogisticRegression, False)\n",
    "housing_price_classif_Y\n",
    "# model(titanic_classification_X, titanic_classification_y, \"Titanic Dataset\", LogisticRegression, False)\n",
    "\n",
    "# from sklearn.svm import SVC\n",
    "# model(sumdata_noise_classi_X, sumdata_noise_classif_Y, \"The Sum Dataset(with noise)\", SVC, False)\n",
    "# model(sumdata_classi_X, sumdata_classif_Y, \"The Sum Dataset(without noise)\", SVC, False)\n",
    "# model(housing_price_classi_X, housing_price_classif_Y, \"Housing Dataset\", SVC, False)\n",
    "# model(titanic_classification_X, titanic_classification_y, \"Titanic Dataset\", SVC, False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
