{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import sklearn.linear_model  as sk  # namespace conflict with pyspark\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import KFold , cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "from helper import *\n",
    "import tensorflow as tf\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "import findspark\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.mllib.regression import LabeledPoint, LinearRegressionWithSGD\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stuff that needs to be done\n",
    "\n",
    "- Find another framework.\n",
    "    - Sklearn\n",
    "    - Spark\n",
    "    - Tensorflow/R\n",
    "- Algorithms\n",
    "    - Linear Regression\n",
    "    - Logistic Regression\n",
    "    - Maybe SVM /Random forest\n",
    "- Find two metrics \n",
    "    - RMSE \n",
    "    - Some other\n",
    "- Do 70/30 split training and 10 cross validation\n",
    "    - only did cross validation\n",
    "- Find another dataset \n",
    "    - Sum data with noise \n",
    "    -  Maybe housing data set\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AABABUTdx7YqCeBquA1Ky7z8a/The%20SUM%20dataset?dl=1#\"\n",
    "housing_price_url = \"https://www.dropbox.com/sh/euppz607r6gsen2/AAAVLZzU4E7ro0BiRzPG3pP8a/House%20Prices?dl=1\"\n",
    "all_urls = [sumdata_url, housing_price_url]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "get_data(all_urls) # retrieves the data if there is no data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sumdata_noise_path = \"data/with noise/The SUM dataset, with noise.csv\"\n",
    "sumdata_path = \"data/without noise/The SUM dataset, without noise.csv\"\n",
    "housing_price_path = \"data/housing dataset.csv\" # has more than 30 features\n",
    "# need one more\n",
    "# what a brilliant idea to name files with space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_chunks = [100, 500, 1000, 5000, 10000, 50000, 100000, 500000,\n",
    "1000000, 5000000, 10000000, 50000000, 100000000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets sum_noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Instance</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "      <th>Feature 3</th>\n",
       "      <th>Feature 4</th>\n",
       "      <th>Feature 5 (meaningless but please still use it)</th>\n",
       "      <th>Feature 6</th>\n",
       "      <th>Feature 7</th>\n",
       "      <th>Feature 8</th>\n",
       "      <th>Feature 9</th>\n",
       "      <th>Feature 10</th>\n",
       "      <th>Noisy Target</th>\n",
       "      <th>Noisy Target Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>62485</td>\n",
       "      <td>58472</td>\n",
       "      <td>84200</td>\n",
       "      <td>86181</td>\n",
       "      <td>75529</td>\n",
       "      <td>136939</td>\n",
       "      <td>150633</td>\n",
       "      <td>230058</td>\n",
       "      <td>246491</td>\n",
       "      <td>257336</td>\n",
       "      <td>1352179</td>\n",
       "      <td>Very Large Number</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Instance  Feature 1  Feature 2  Feature 3  Feature 4  \\\n",
       "0         1      62485      58472      84200      86181   \n",
       "\n",
       "   Feature 5 (meaningless but please still use it)  Feature 6  Feature 7  \\\n",
       "0                                            75529     136939     150633   \n",
       "\n",
       "   Feature 8  Feature 9  Feature 10  Noisy Target Noisy Target Class  \n",
       "0     230058     246491      257336       1352179  Very Large Number  "
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumdata_noise = pd.read_csv(sumdata_noise_path, delimiter=\";\")\n",
    "sumdata_noise.head(n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess sum_noise dataset\n",
    "\n",
    "- Remove 'Instance' as it simply represents the row number\n",
    "- Extract 'Nosiy Target' as regression target\n",
    "- Extract 'Nosiy Class' as classification target\n",
    "- Extract rest columns as explananatory variables\n",
    "- Apply Feature Scaling to the dataset \n",
    "\n",
    "- Ensure all dataframe has been converted to numpy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create same base line for Spark, Keras, Sklearn implementation\n",
    "- Using the same feature scaled data for all implementation\n",
    "- Same number iterations\n",
    "- Same cost function for minimising(for gradient descent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove 'Instance' as it simply represents the row number\n",
    "\n",
    "\n",
    "\n",
    "sumdata_noise.drop('Instance', axis = 1)\n",
    "\n",
    "# Extract 'Nosiy Target' as regression target\n",
    "sumdata_noise_reg_Y = sumdata_noise.loc[:, ['Noisy Target']]\n",
    "# Extract 'Nosiy Target Class' as regression target\n",
    "sumdata_noise_classif_Y = sumdata_noise.loc[:, ['Noisy Target Class']]\n",
    "\n",
    "# Extract rest columns as explananatory variables\n",
    "sumdata_noise_X =sumdata_noise.drop(['Instance','Noisy Target Class'\n",
    "                                    ], axis = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling\n",
    "- Keeping them as dataframes (transform returns ndarray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "scX = StandardScaler()\n",
    "scY = StandardScaler()\n",
    "columns = sumdata_noise_X.columns\n",
    "\n",
    "sumdata_noise_X[columns] = scX.fit_transform(sumdata_noise_X)\n",
    "columns = sumdata_noise_reg_Y.columns\n",
    "sumdata_noise_reg_Y[columns] = scX.fit_transform(sumdata_noise_reg_Y)\n",
    "\n",
    "\n",
    "sumdata_noise_scaled = pd.concat([sumdata_noise_X,sumdata_noise_reg_Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression using pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-9-8030b2fc72d9>:1 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-8030b2fc72d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/jibin/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m/home/jibin/anaconda3/lib/python3.6/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    297\u001b[0m                         \u001b[0;34m\" created by %s at %s:%s \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 299\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    300\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-9-8030b2fc72d9>:1 "
     ]
    }
   ],
   "source": [
    "sc = pyspark.SparkContext()\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from  pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def diff_sq(n):\n",
    "    return (n[0] - n[1])**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sum_1(n,n2):\n",
    "    return (n + n2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def spark_linear_regression(data_df):\n",
    "    \"\"\"\n",
    "        This doesnt have cross validation \n",
    "        doesnt have test split\n",
    "    \"\"\"\n",
    "    input_features = list(sumdata_noise.columns)[:-2] \n",
    "    \n",
    "    data = data_df.select(input_features)\\\n",
    "      .rdd\\\n",
    "      .map(lambda line:LabeledPoint(line[-1],line[1:-1]))\n",
    " \n",
    "    \n",
    "    lr = LinearRegressionWithSGD( )\n",
    "\n",
    "    # Fit 2 models, using different regularization parameters\n",
    "    model = lr.train(data,iterations=1000)\n",
    "    \n",
    "    # Evaluate the model on training data\n",
    "    valuesAndPreds =data.map(lambda p: (p.label, model.predict(p.features))) \n",
    "     \n",
    "    MSE = valuesAndPreds\\\n",
    "        .map(diff_sq)\\\n",
    "        .reduce(sum_1) / valuesAndPreds.count()\n",
    "    print(\"Mean Squared Error = \" + str(sqrt(MSE)))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jibin/anaconda3/lib/python3.6/site-packages/pyspark/mllib/regression.py:281: UserWarning: Deprecated in 2.0.0. Use ml.regression.LinearRegression.\n",
      "  warnings.warn(\"Deprecated in 2.0.0. Use ml.regression.LinearRegression.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error = 0.2141731306672613\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.2336466274817069\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.24004858367273027\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.242831777382388\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.24331256292359593\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.24238415215080497\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.242741461062573\n",
      "\n",
      "\n",
      "Mean Squared Error = 0.244050615169765\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for chunk in data_chunks:\n",
    "\n",
    "    if chunk > sumdata_noise.shape[0]: # if chunk is greater than the no. of examples\n",
    "        break\n",
    "        \n",
    "        \n",
    "    data = sumdata_noise[:chunk]\n",
    "    data = data.drop('Noisy Target Class', axis=1)\n",
    "    columns = data.columns\n",
    "    \n",
    "    data[columns] = StandardScaler().fit_transform(data)\n",
    "    data_df = sqlContext.createDataFrame(data)\n",
    "     \n",
    "    \n",
    "    spark_linear_regression(data_df)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sklearn_linear_regression_sgd( X_train, X_test, y_train, y_test): \n",
    "    \"\"\"\n",
    "        This might not be much of use as I wasn't able to control \n",
    "        the learning rate, number of iterations\n",
    "    \"\"\"\n",
    "    clf = sk.SGDRegressor()\n",
    "    model = clf.fit(X_train, y_train.ravel())\n",
    "    preds = model.predict(X_test)\n",
    "    return mean_squared_error(y_test, preds)**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_test_cross( data_x, data_y, dataset_name): \n",
    "    \n",
    "    errors = np.zeros((3, 10)) # \n",
    "    for chunk in data_chunks:\n",
    "\n",
    "        if chunk > sumdata_noise.shape[0]: # if chunk is greater than the no. of examples\n",
    "            break\n",
    "\n",
    "        X = data_x.head(n = chunk)\n",
    "        y = data_y.head(n = chunk)\n",
    "        \n",
    "        X = X.as_matrix()\n",
    "        y = y.as_matrix() \n",
    "        kf = KFold(n_splits=10)\n",
    "        \n",
    "        \n",
    "        for (i, (train_index, test_index)) in enumerate(kf.split(X)):\n",
    "            \n",
    "            \n",
    "            X_train, X_test = X[train_index], X[test_index]\n",
    "            y_train, y_test = y[train_index], y[test_index]\n",
    "            # sklearn storing \n",
    "            errors[0][i] = sklearn_linear_regression_sgd( X_train, X_test, y_train, y_test)\n",
    "            #\n",
    "            #errors[0][1] = sklearn_linear_regression_sgd( X_train, X_test, y_train, y_test)\n",
    "        \n",
    "        # calculates mean across rows\n",
    "        mean_error = np.mean(errors, axis=0) \n",
    "        print (\"Data set: {} Chunk Size: {} Error: {}\".format(dataset_name,chunk, mean_error[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data set: Sum data with noise Chunk Size: 100 Error: 0.02240438583606064\n",
      "Data set: Sum data with noise Chunk Size: 500 Error: 0.034185244683955514\n",
      "Data set: Sum data with noise Chunk Size: 1000 Error: 0.03361810209604333\n",
      "Data set: Sum data with noise Chunk Size: 5000 Error: 0.02639941556864127\n",
      "Data set: Sum data with noise Chunk Size: 10000 Error: 0.020960751781316834\n",
      "Data set: Sum data with noise Chunk Size: 50000 Error: 0.00636657752448506\n",
      "Data set: Sum data with noise Chunk Size: 100000 Error: 0.002108051672184291\n",
      "Data set: Sum data with noise Chunk Size: 500000 Error: 0.0002771586887816033\n"
     ]
    }
   ],
   "source": [
    "model_test_cross( sumdata_noise_X, sumdata_noise_reg_Y, \"Sum data with noise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = np.array([[1,3], [5,6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r =np.mean(n\n",
    "    , axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
